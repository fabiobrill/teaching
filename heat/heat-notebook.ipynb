{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a665c97",
   "metadata": {},
   "source": [
    "### Learning an Emulator Model for Urban Heat Prediction\n",
    "\n",
    "This notebook demonstrates step-by-step how to train a machine learning regression algorithm on spatial data, to use the distribution of green, blue, and grey elements for the prediction of heat - based on a target layer from a physical climate model. Technically, we attempt to \"emulate\" the climate model behavior by a data-driven approach. The model is evaluated regarding a performance metric, and then used for predicting scenarios of altered landcover.\n",
    "\n",
    "The workflow contains the following steps:\n",
    "1. Reading green-blue-grey elements as vector data and conversion to raster\n",
    "2. Feature engineering, namely distance-to and density-of the elements\n",
    "3. Stacking all layers, and spatial tiling\n",
    "4. Sampling training data (nested cross-validataion)\n",
    "5. Parameter grid search for a RandomForestRegressor\n",
    "6. Performance evaluation of best model\n",
    "7. Model inspection by feature importance and partial dependence plots\n",
    "8. Spatial prediction of full layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed4710-3ee7-4b16-b192-4ad2fa4efb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rioxarray\n",
    "import rasterio\n",
    "from rasterio.features import rasterize\n",
    "from scipy.ndimage import convolve\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from customFunctions import writeRaster\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff631e18",
   "metadata": {},
   "source": [
    "#### 1. Reading vector data and conversion to raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b12d96-5d24-4907-ac5c-8c1fce26cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = \"./data\"\n",
    "os.listdir(datafolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e30f24-8493-41b0-b7b8-dad33528fbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An output layer from the climate model is used as reference for rasterizing the vector data\n",
    "reference_raster = rioxarray.open_rasterio(datafolder + \"/T2M_daily_mean_max_topography_2011_2020_present_30.tif\")\n",
    "reference_raster.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6724d5a-7dfc-4766-a212-508a2b34543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_transform = reference_raster.rio.transform()\n",
    "ref_crs = reference_raster.rio.crs\n",
    "# print the CRS and transform to check\n",
    "print(\"CRS of the reference raster:\", ref_crs)\n",
    "print(\"Transform of the reference raster:\", ref_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fa3e7d-52e1-4a98-83da-9f7e31c24433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets to rasterize\n",
    "railways = gpd.read_file(datafolder + \"/osm_railways_without_subway.gpkg\")\n",
    "subway = gpd.read_file(datafolder + \"/osm_subway.gpkg\")\n",
    "roads = gpd.read_file(datafolder + \"/osm_major_roads.gpkg\")\n",
    "water = gpd.read_file(datafolder + \"/osm_water.gpkg\")\n",
    "greens = gpd.read_file(datafolder + \"/osm_green_without_forest.gpkg\")\n",
    "forest = gpd.read_file(datafolder + \"/osm_forest.gpkg\")\n",
    "buildings = gpd.read_file(datafolder + \"/osm_buildings.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a9d9d-d2b2-41a3-8a78-9351ca39bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 'dictionary' for iterating over the datasets\n",
    "datasets = {\n",
    "    \"railways\": railways,\n",
    "    \"subway\": subway,\n",
    "    \"roads\": roads,\n",
    "    \"water\": water,\n",
    "    \"greens\": greens,\n",
    "    \"forest\": forest,\n",
    "    \"buildings\": buildings\n",
    "}\n",
    "for name, dataset in datasets.items():\n",
    "    # rasterize the dataset\n",
    "    rasterized = rasterize(\n",
    "        [(geom, 1) for geom in dataset.geometry],\n",
    "        out_shape=reference_raster.shape[1:],\n",
    "        transform=ref_transform,\n",
    "        fill=0,\n",
    "        all_touched=True\n",
    "    )\n",
    "    \n",
    "    # write the rasterized dataset to a GeoTIFF file\n",
    "    output_filename = os.path.join(datafolder, f\"{name}_raster.tif\")\n",
    "    writeRaster(rasterized, output_filename, ref_crs=ref_crs, ref_transform=ref_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bbaeba",
   "metadata": {},
   "source": [
    "#### 2. Feature engineering\n",
    "\n",
    "Here we start by defining functions that transform the rasters. Distances and focal mean values will provide spatial context information to the learning algorithm that we want to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03caa7ff-849b-4338-b20d-0ab5280bf1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute distance to nearest feature\n",
    "def compute_distance_to_nearest_feature(raster, feature_name):\n",
    "    # Invert the raster to get the distance to the nearest feature\n",
    "    inverted_raster = np.where(raster > 0, 0, 1)\n",
    "    # Compute the distance transform\n",
    "    distance_raster = distance_transform_edt(inverted_raster)\n",
    "    # multiply by the grid size (30m) to convert to meters\n",
    "    distance_raster *= 30  # Assuming each grid cell is 30m x 30m\n",
    "    # Save the distance raster to a new GeoTIFF file\n",
    "    output_filename = os.path.join(datafolder, f\"distance_to_nearest_{feature_name}.tif\")\n",
    "    crs = raster.rio.crs\n",
    "    transform = raster.rio.transform()\n",
    "    writeRaster(distance_raster, output_filename, crs=crs, transform=transform)\n",
    "\n",
    "def compute_convolution(raster, feature_name, kernel_size=33):\n",
    "    kernel = np.ones((kernel_size, kernel_size), dtype=np.float32) / (kernel_size * kernel_size)\n",
    "    # Perform convolution with the defined kernel\n",
    "    convolved_raster = convolve(raster, kernel, mode='constant', cval=0.0)\n",
    "    #convolved_datasets[name] = convolved_raster\n",
    "    # Save the convolved raster to a new GeoTIFF file\n",
    "    output_filename = os.path.join(datafolder, f\"{feature_name}_convolved_{kernel_size*30}m.tif\")\n",
    "    crs = raster.rio.crs\n",
    "    transform = raster.rio.transform()\n",
    "    writeRaster(convolved_raster, output_filename, crs=crs, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b4b0d1-3525-41ae-adfb-07c7dd58a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rasterized datasets, and ensure they are in float32 format\n",
    "greens_raster = rioxarray.open_rasterio(os.path.join(datafolder, \"greens_raster.tif\")).squeeze().astype(np.float32)\n",
    "forest_raster = rioxarray.open_rasterio(os.path.join(datafolder, \"forest_raster.tif\")).squeeze().astype(np.float32)\n",
    "water_raster = rioxarray.open_rasterio(os.path.join(datafolder, \"water_raster.tif\")).squeeze().astype(np.float32)\n",
    "railways_raster = rioxarray.open_rasterio(os.path.join(datafolder, \"railways_raster.tif\")).squeeze().astype(np.float32)\n",
    "subway_raster = rioxarray.open_rasterio(os.path.join(datafolder, \"subway_raster.tif\")).squeeze().astype(np.float32)\n",
    "roads_raster = rioxarray.open_rasterio(os.path.join(datafolder, \"roads_raster.tif\")).squeeze().astype(np.float32)\n",
    "buildings_raster = rioxarray.open_rasterio(os.path.join(datafolder, \"buildings_raster.tif\")).squeeze().astype(np.float32)\n",
    "\n",
    "# loop through the datasets, compute the convolution and save them\n",
    "datasets = {\n",
    "    \"greens\": greens_raster,\n",
    "    \"forest\": forest_raster,\n",
    "    \"water\": water_raster,\n",
    "    \"railways\": railways_raster,\n",
    "    \"subway\": subway_raster,\n",
    "    \"roads\": roads_raster,\n",
    "    \"buildings\": buildings_raster\n",
    "}\n",
    "\n",
    "# Loop through the datasets and compute distance to / density of\n",
    "# greens, forest, water, railways, subway, roads and buildings\n",
    "for name, raster in datasets.items():\n",
    "    compute_distance_to_nearest_feature(raster, name)\n",
    "    compute_convolution(raster, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487f5782",
   "metadata": {},
   "source": [
    "#### 3. Stacking and tiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6ad216-0c98-4a6e-9797-a5b6a8089bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customFunctions import slice_into_tiles, save_tiles_as_geotiff, save_stacked_raster_as_geotiff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71c9c1-0bbd-4bfc-80ec-d1ed321a4347",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_raster = reference_raster.copy().squeeze()\n",
    "heat_raster.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c323dc-ea1f-4167-901c-3195b70c0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the convolved feature rasters\n",
    "greens_convolved= rioxarray.open_rasterio(os.path.join(datafolder, \"greens_convolved_990m.tif\")).squeeze()\n",
    "forest_convolved= rioxarray.open_rasterio(os.path.join(datafolder, \"forest_convolved_990m.tif\")).squeeze()\n",
    "water_convolved= rioxarray.open_rasterio(os.path.join(datafolder, \"water_convolved_990m.tif\")).squeeze()\n",
    "railways_convolved= rioxarray.open_rasterio(os.path.join(datafolder, \"railways_convolved_990m.tif\")).squeeze()\n",
    "subway_convolved= rioxarray.open_rasterio(os.path.join(datafolder, \"subway_convolved_990m.tif\")).squeeze()\n",
    "roads_convolved= rioxarray.open_rasterio(os.path.join(datafolder, \"roads_convolved_990m.tif\")).squeeze()\n",
    "buildings_convolved= rioxarray.open_rasterio(os.path.join(datafolder, \"buildings_convolved_990m.tif\")).squeeze()\n",
    "\n",
    "# load the distance feature rasters\n",
    "greens_distance = rioxarray.open_rasterio(os.path.join(datafolder, \"distance_to_nearest_greens.tif\")).squeeze()\n",
    "forest_distance = rioxarray.open_rasterio(os.path.join(datafolder, \"distance_to_nearest_forest.tif\")).squeeze()\n",
    "water_distance = rioxarray.open_rasterio(os.path.join(datafolder, \"distance_to_nearest_water.tif\")).squeeze()\n",
    "railways_distance = rioxarray.open_rasterio(os.path.join(datafolder, \"distance_to_nearest_railways.tif\")).squeeze()\n",
    "subway_distance = rioxarray.open_rasterio(os.path.join(datafolder, \"distance_to_nearest_subway.tif\")).squeeze()\n",
    "roads_distance = rioxarray.open_rasterio(os.path.join(datafolder, \"distance_to_nearest_roads.tif\")).squeeze()\n",
    "buildings_distance = rioxarray.open_rasterio(os.path.join(datafolder, \"distance_to_nearest_buildings.tif\")).squeeze()\n",
    "\n",
    "# stack them all, with heat as the first layer\n",
    "stacked_raster = np.stack([heat_raster, greens_convolved, forest_convolved,\n",
    "                           water_convolved, railways_convolved, subway_convolved,\n",
    "                           roads_convolved, buildings_convolved,\n",
    "                           greens_distance, forest_distance, water_distance,\n",
    "                           railways_distance, subway_distance, roads_distance,\n",
    "                           buildings_distance], axis=0)\n",
    "stacked_raster.shape # should be (15, height, width) where 15 is the number of layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74a2af-a4b7-4012-8a4b-ffedad564b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice the stacked raster into 4 tiles, using the provided custom function\n",
    "tile_size = stacked_raster.shape[1] // 2  # Assuming we want 2x2 tiles\n",
    "tiles = slice_into_tiles(stacked_raster, tile_size)\n",
    "tiles.shape\n",
    "\n",
    "# the geolocation of the tiles is based on the original raster's transform and CRS\n",
    "# but we need to adjust the transform for each tile\n",
    "output_folder = os.path.join(datafolder, \"tiles\")\n",
    "save_tiles_as_geotiff(tiles, heat_raster.rio.transform(), heat_raster.rio.crs, output_folder)\n",
    "\n",
    "# alternative version: save full stacked raster as a single GeoTIFF\n",
    "output_filename = os.path.join(datafolder, \"stacked_raster.tif\")\n",
    "save_stacked_raster_as_geotiff(stacked_raster, heat_raster.rio.transform(), heat_raster.rio.crs, output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7eb169",
   "metadata": {},
   "source": [
    "#### 4. Sampling training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43fd657-f0a2-485e-87fb-765810e6c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Import custom functions from another Python script in the same folder\n",
    "from customFunctions import readRaster, writeRaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0addd837-361d-405b-8b7e-0374884b341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb76633-2410-498e-a937-c072616b55b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, transform, crs, height, width = readRaster(os.path.join(datafolder, \"tiles/tile_0.tif\"))\n",
    "training_data = data.reshape(data.shape[0], -1).T  # Reshape to (num_pixels, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af7da0-76d6-445f-a3be-6be6ba2ad99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0974f-59ac-46fc-b300-6e13dc5881fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_layer = training_data[:, 0]  # First layer is heat\n",
    "features = training_data[:, 1:]   # Remaining layers are features\n",
    "\n",
    "# Randomly select a fraction of the pixels for training, e.g. 0.1 for 10%\n",
    "fraction = 0.10\n",
    "num_pixels = features.shape[0]\n",
    "sample_size = int(num_pixels * fraction)\n",
    "random_indices = random.sample(range(num_pixels), sample_size)\n",
    "X_sample = features[random_indices]    # the predictors (features)\n",
    "y_sample = heat_layer[random_indices]  # the target variable (heat layer)\n",
    "\n",
    "# Split the sample into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5c760",
   "metadata": {},
   "source": [
    "#### 5. Parameter grid search\n",
    "\n",
    "This example is for training a RandomForestRegressor, which is an ensemble of decision trees. Details on the algorithm and its hyperparameters can be looked up, for example, on the scikit-learn website. It is an established method, famous for yielding good results on many datasets without requiring much preprocessing (e.g. no scaling needed), and for being less sensitive to sub-optimal parametrization than other methods. For this exercise, we will only try out a very narrow range for a few parameters. For other algorithms, doing a proper parametrization can be crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9dad6-37a5-42f7-a798-cde790981cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the computational effort increases drastically with the number of estimators \n",
    "# and with the combinations of hyperparameters to try out\n",
    "param_grid = {'n_estimators': [10, 50],    # how many decision trees the ensemble consists of\n",
    "              'max_depth': [3, 10, None],  # number of decision layers within each tree\n",
    "              'min_samples_split': [2, 5], # stop splitting a node if it has less than this many samples\n",
    "              'n_jobs': [-1]}              # Use all available cores for parallel processing\n",
    "# Create a Random Forest Regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "# print the ranking of estimators with hyperparameters and skill score as table\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "print(results[['params', 'mean_test_score', 'rank_test_score']].sort_values(by='rank_test_score'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b35363",
   "metadata": {},
   "source": [
    "#### 6. Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecb44f-81a1-4b05-abe9-d3c5dc39aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customFunctions import plotSideBySide, plotSpatialError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbacc279-b581-4ed7-a5fc-41dc7bd9723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=10, max_depth=None, n_jobs=-1, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"Model R^2 score: {score:.4f}\")\n",
    "# predict the heat layer using the model\n",
    "predicted_heat = model.predict(features)\n",
    "# reshape the predicted heat back to the original tile shape\n",
    "predicted_heat_reshaped = predicted_heat.reshape(height, width)\n",
    "# use the custom function to plot the original and predicted heat layers side by side\n",
    "plotSideBySide(data[0], predicted_heat_reshaped)\n",
    "plotSpatialError(data[0], predicted_heat_reshaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9e9b8-16a7-437a-a2ce-d8f30fb7f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib allows saving and loading of trained models\n",
    "import joblib\n",
    "\n",
    "output_path = os.path.join(datafolder, \"predicted_heat_tile_0_rf.tif\")\n",
    "writeRaster(predicted_heat_reshaped, output_path, transform, crs)\n",
    "# save the fitted model to a file\n",
    "model_path = os.path.join(datafolder, \"random_forest_model.pkl\")\n",
    "joblib.dump(model, model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3084826",
   "metadata": {},
   "source": [
    "We can now test the performance of our trained model on a different tile, i.e. data that it hasn't seen during trainnig, and that is also spatially independent of the training data! This should tell us whether the model learned any transferable patterns from the data, or whether it simply overfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ae8307-9c14-481a-82e1-4cb6add31d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_path_1 = os.path.join(datafolder, \"tiles/tile_1.tif\")\n",
    "tile_1, transform_1, crs_1, height_1, width_1 = readRaster(tile_path_1)\n",
    "tile_data_reshaped_1 = tile_1.reshape(tile_1.shape[0], -1).T  # (num_pixels, num_layers)\n",
    "predicted_heat_tile_1 = model.predict(tile_data_reshaped_1[:, 1:])  # Use features only\n",
    "predicted_heat_reshaped_1 = predicted_heat_tile_1.reshape(height_1, width_1)\n",
    "# evaluate the model on tile 1\n",
    "mse = mean_squared_error(tile_1[0].flatten(), predicted_heat_reshaped_1.flatten())\n",
    "r2 = r2_score(tile_1[0].flatten(), predicted_heat_reshaped_1.flatten())\n",
    "print(f\"Mean Squared Error for Tile 1: {mse:.4f}\")\n",
    "print(f\"R^2 Score for Tile 1: {r2:.4f}\")\n",
    "# plot the original and predicted heat layers side by side\n",
    "plotSideBySide(tile_1[0], predicted_heat_reshaped_1, title1=\"Original Heat Layer Tile 1\", title2=\"Predicted Heat Layer Tile 1\")\n",
    "plotSpatialError(tile_1[0], predicted_heat_reshaped_1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995bc45",
   "metadata": {},
   "source": [
    "#### 7. Model inspection\n",
    "\n",
    "A linear regression model comes with weights that tell how important the individual predictive features (X1, X2, ...) are to the prediction (y-pred). RandomForest is a strongly non-linear ensemble method, which cannot be interpreted that easily. However, several method exist to derive a \"feature imporance\", e.g. by permutation or from the average position of the respective features within the individual decision trees. The details can be found in the documentation, but are outside the scope of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b385842-0637-492e-ae0c-71abc948d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_names = ['greens_convolved', 'forest_convolved', 'water_convolved',\n",
    "                  'railways_convolved', 'subway_convolved', 'roads_convolved',\n",
    "                  'buildings_convolved', 'greens_distance', 'forest_distance',\n",
    "                  'water_distance', 'railways_distance', 'subway_distance',\n",
    "                  'roads_distance', 'buildings_distance']\n",
    "\n",
    "# Visualize the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(feature_importances)), feature_importances)\n",
    "plt.xticks(range(len(feature_importances)), features_names, rotation=45)\n",
    "plt.title(\"Feature Importances of Random Forest Model\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Value\")\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda402f",
   "metadata": {},
   "source": [
    "While the feature importance gives an idea of how much the model makes use of particular features, this does not really tell anything about the direction of influence! When trying to understand the model behavior, we usually want an answer to questions like \"Is more or less heat predicted in areas of high building density?\", or even \"how far from the forest edge does the model still predict a cooling effect?\". Such insights can be derived from partial dependence plots. These plots visualize the marginal effect of a single feature on the prediction (all other feature values held constant). Keep in mind, though, that there can be interaction effects which even out in the marginal effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41db377-2672-4684-9b3f-d642eea8a248",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 18))\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    model,\n",
    "    X_train,\n",
    "    features=range(len(features_names)),\n",
    "    feature_names=features_names,\n",
    "    ax=ax,\n",
    "    grid_resolution=200\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3033eb5c",
   "metadata": {},
   "source": [
    "#### 8. Prediction of the full layer\n",
    "\n",
    "with the model stored in a file, and its behavior roughly understood, we can proceed to apply the model on the full spatial extent and/or different future scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f1440a-8fc7-4354-8729-c967bfc813d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customFunctions import predictHeatLayer\n",
    "help(predictHeatLayer) # check the docstring of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d3ca65-b988-447c-9eac-2cf34498034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = joblib.load(os.path.join(datafolder, \"random_forest_model.pkl\"))\n",
    "predictHeatLayer(datafolder + \"/stacked_raster.tif\", rf_model, datafolder + \"/prediction_subset.tif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00386062-fe75-4e7e-b660-94141b252ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
