{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 - XGBoost and SHAP for drought impact analysis**\n",
    "\n",
    "In this exercise we - again - use data of drought-related indicators on agricultural fields, namely\n",
    "- SPEI : Standarized Precipitation-Evaporation Index\n",
    "- SMI : Soil Moisture Index\n",
    "- LST/NDVI : Land Surface Temperature / Normalized Difference Vegetation Index\n",
    "- AZL : Ackerzahl (soil quality)\n",
    "- TWI : Topographic Wetness Index\n",
    "- NFK : plant available water (*nutzbare Feldkapazität*)\n",
    "\n",
    "In addition to the Python modules that you already know from last session, we now also need the machine learning libraries *scikit-learn*, *xgboost*, and *shap*. For data manipulation we use *numpy* and *pandas* instead of geopandas. The dataset is identical just without the geometry (.csv instread of .gpkg)\n",
    "\n",
    "*scikit-learn* is abbreviated *sklearn* and contains a wide range of algorithms as well as data preparation routines, scoring metrics, and even model inspection techniques. Only a few functions are needed here, so we import them explicitly. More information on the included methods can be found on the website: https://scikit-learn.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "\n",
    "pd.set_option('display.max_colwidth', 255) # let pandas print full output of tables\n",
    "os.listdir(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "The next two code blocks load the data and print the columns to check what it contains. Then data is reduced to the colums of interest for this exercise by typing the column names in double square brackets [[]]. Samples with missing values in any of the retained columns are excluded by *.dropna()*\n",
    "\n",
    "*Crop* is a categorical variable of crop types. It can be used as such, but many algorithms are not designed to handle categorical variables. A workaround is to converted them to \"one-hot encoding\", meaning that each category is transferred to a separate binary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/subset_random.csv\")\n",
    "print(data.shape, data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns of interest\n",
    "data = data[[\"crop\", \"lstndvi_anom\", \"azl\", \"twi\", \"nfk\", \"SPEI_magnitude\", \"SMI_magnitude\"]]\n",
    "\n",
    "# remove all rows that contain missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# convert crop type to binary columns (one-hot encoding)\n",
    "data = pd.get_dummies(data, columns=[\"crop\"])\n",
    "\n",
    "# store the names of all crop columns as an object to easily access them\n",
    "crop_cols = [col for col in data.columns if col.startswith('crop_')]\n",
    "\n",
    "# print the first rows of the modified data table\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "\n",
    "The target vector of a regression is called **y**\n",
    "\n",
    "The matrix of predictive features is called **X**\n",
    "\n",
    "In the following setup, the crop health indicator *lstndvi_anom* is assigned as target and all other columns as predictive features. An implementation detail of XGBoost is that it requires input data in *numpy* format (as opposed to *scikit-learn* internal algorithms that can handle *pandas* objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"lstndvi_anom\"]\n",
    "X = data.drop(\"lstndvi_anom\", axis=\"columns\") # everything except lstndvi_anom\n",
    "\n",
    "# Here would be an option to select only a subset of features\n",
    "#X = data.drop(\"lstndvi_anom\", axis=\"columns\").drop(crop_cols, axis=\"columns\")\n",
    "#X_featureset1 = data[[\"SPEI_magnitude\"]]\n",
    "#X_featureset2 = data[[\"SPEI_magnitude\", \"azl\"]]\n",
    "# ...\n",
    "\n",
    "# convert to numpy for xgb - and revert this later to get feature names in SHAP plots\n",
    "Xnp = X.to_numpy()\n",
    "ynp = y.to_numpy()\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Algorithm\n",
    "\n",
    "*XGBoost* can be used for regression or classification. Here the *XGBoostRegressor* is used. It comes with many hyperparameters (i.e. parameters of the algorithm, as opposed to parameters of the fitted model). For these hyperparameters there are default settings that can be looked up by printing the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = xgb.XGBRegressor()\n",
    "algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One very important thing that is often overlooked by ML-beginners and even intermediate level articles & tutorials, is the algorithm-internal objective function.** ML models are crated from algorithms by optimizing a function in which performance is measured by a defined metric. That means the structure of the resulting model is adjusted to perform as good as possible on this very metric. When evaluating different models, it might make sense to compare them by a differnt metric that is related to the specifics of the intended application case. However, keep in mind that you might then be looking at metrics on which the model has not been optimized!\n",
    "\n",
    "In the case of *XGBoostRegressor*, the default metric is the squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm.objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How **NOT** to do it\n",
    "\n",
    "In principle we could now simply fit the algorithm to the data to obtain a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = algorithm.fit(X,y)\n",
    "\n",
    "# print the R² score (coefficient of determination)\n",
    "r2_score(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is overly optimistic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling & Parametrization\n",
    "\n",
    "A key concept of machine learning is that model skill needs to be assessed on \"unseen\" data, i.e. data not used during training. One reason for this is that strong algorithms can easily learn to perfectly fit complex training data - however, they might overfit rather than generalize, and consequently perform poor in application. Hyperparameters of the algorithms can be tuned to adjust the resulting model complexity (bias-variance tradeoff). Choosing good hyperparameters semi-automatically can be done by ranking models based on their performance on independent test data.\n",
    "\n",
    "A common way to implement this is via cross-validation: data is subdivided into n sets, of which n-1 are used for training and the remaining 1 for testing. In the case of a 5-fold cross-validation that would mean 80% of data is used for training and 20% for testing, repeated 5 times.\n",
    "\n",
    "However, it is good practise to assess the skill of the final model on yet another independent holdout set, that has not been used in the model tuning at all. Such a setup is termed nested cross-validation.\n",
    "\n",
    "Note that in reality there might be stronger violations of the independence assumption than those caused by imperfect computational sampling. For example data from a small region or short period of time might not capture all dynamics of interest, no matter how the collected data is subdivided. In the case of this exercise, the data is from Brandenburg and covers the last 10 years. It does not contain all information on drought impacts \"in general\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nested cross validataion](nested_cv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically these subsets are called 'training' and 'test', but for the sake of clarity let's call them 'holdout' and 'parametrization' here, as they refer to the outer loop of the sketched workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function train_test_split returns 4 objects. These can be stored by assigning 4 variables\n",
    "X_parametrization, X_holdout, y_parametrization, y_holdout = train_test_split(Xnp, ynp, test_size=0.3)\n",
    "\n",
    "# change the \"test_size\" parameter and check the dimensionality of the parametrizations (rows, columns)\n",
    "print(X_parametrization.shape, y_parametrization.shape, X_holdout.shape, y_holdout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block implements a 5-fold cross-validation during which different values for 2 parameters are tested. The resulting R² skill score on the \"test\" part of the data is used to create a ranking of parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some potential values for two parameters of interest\n",
    "xgb_parameters = {\n",
    "    'n_estimators':[10,50,100],\n",
    "    'max_depth':[3,5,7]\n",
    "    }\n",
    "\n",
    "# parametrization internally using another data split, controlled by the parameter cv\n",
    "gs = GridSearchCV(algorithm, param_grid=xgb_parameters, cv=5, scoring=\"r2\")\n",
    "gs_results = gs.fit(X_parametrization, y_parametrization)\n",
    "\n",
    "# print the results in human-readible table format\n",
    "pd.DataFrame({'rank': gs_results.cv_results_['rank_test_score'],\n",
    "              'mean': gs_results.cv_results_['mean_test_score'],\n",
    "              'sd'  : gs_results.cv_results_['std_test_score'],\n",
    "              'params': gs_results.cv_results_['params']}).sort_values(\"rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model from this grid search is now re-trained using the entire parametrization subset, and then used to predict the holdout set. Note how much lower the score is than for the naive model fit on the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs_results.best_estimator_.fit(X_parametrization, y_parametrization)\n",
    "r2_score(y_holdout, best_model.predict(X_holdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization by early-stopping**\n",
    "\n",
    "*XGBoost* has another way of adjusting model complexity: the maximum number of decision trees (boosted rounds) is controlled by the *n_estimators* parameter, but this maximum number might not be required. If there is no improvement of skill in a specified number of learning steps, the training can be stopped early, i.e. with a lower number of trees. This usually results in a less complex model, that might generalize better to unseen data, as tested on the holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.set_params(early_stopping_rounds=2)\n",
    "best_model.fit(X_parametrization, y_parametrization, eval_set=[(X_holdout, y_holdout)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_holdout, best_model.predict(X_holdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatization: wrap everything in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(algorithm, param_grid, X, y, holdout=0.3, repetitions=10, ncv=5):\n",
    "    \"\"\"\n",
    "    wrapper for repeated model fitting with random train test split, including the \n",
    "    computation of performance metrics on holdout set\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize empty lists to store the results\n",
    "    cvscores = []\n",
    "    holdoutscores = []\n",
    "    paramlist = []\n",
    "    estimatorlist = []\n",
    "\n",
    "    # convert to numpy for xgb - and revert this later to get feature names in SHAP plots\n",
    "    Xnp = X.to_numpy()\n",
    "    ynp = y.to_numpy()\n",
    "    \n",
    "    for i in range(repetitions):\n",
    "        print(i)\n",
    "\n",
    "        # outer loop for validation\n",
    "        X_parametrization, X_holdout, y_parametrization, y_holdout = train_test_split(Xnp, ynp, test_size=holdout)\n",
    "        \n",
    "        # inner loop to optimize the hyperparameters\n",
    "        gs = GridSearchCV(algorithm, param_grid=param_grid, cv=ncv, scoring=\"r2\")\n",
    "        gs_results = gs.fit(X_parametrization, y_parametrization)\n",
    "\n",
    "        # store the results in separate list\n",
    "        cvscores.append(gs_results.best_score_)\n",
    "        paramlist.append(gs_results.best_params_)\n",
    "        estimatorlist.append(gs_results.best_estimator_)\n",
    "\n",
    "        # re-train best estimator on full training set and compute score on holdout set\n",
    "        best_model_of_iteration = gs_results.best_estimator_.fit(X_parametrization, y_parametrization)\n",
    "        holdoutscores.append(r2_score(y_holdout, best_model_of_iteration.predict(X_holdout)))\n",
    "    \n",
    "    # take the best model of all iterations by scored on holdout set, \n",
    "    # active early stopping and fit on entire data of the inner loop\n",
    "    best_model = estimatorlist[np.argmax(holdoutscores)]\n",
    "    best_model.set_params(early_stopping_rounds=2)\n",
    "    best_model.fit(X_parametrization, y_parametrization, eval_set=[(X_holdout, y_holdout)])\n",
    "    n_stop = best_model.get_booster().num_boosted_rounds()\n",
    "    best_model.set_params(n_estimators = n_stop, early_stopping_rounds=None)\n",
    "    \n",
    "    return(best_model, cvscores, holdoutscores, paramlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the algorithms with some fixed settings that will not be changed during parametrization\n",
    "algorithm = xgb.XGBRegressor(booster='gbtree', tree_method='hist')\n",
    "\n",
    "# define a set of parameter values to check\n",
    "xgb_parameters = {\n",
    "    'n_estimators':[100],\n",
    "    'max_depth':[4,6,8,10],\n",
    "    'gamma':[0, 0.1, 1],\n",
    "    'subsample':[0.8],\n",
    "    'colsample_bytree':[0.8],\n",
    "    'learning_rate':[0.1]\n",
    "    }\n",
    "\n",
    "best_model, cvscores, holdoutscores, paramlist = train(algorithm, xgb_parameters, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostic plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotScores(holdoutscores, cvscores, plottitle):\n",
    "    sns.distplot(holdoutscores, label=\"holdout\", color=\"tomato\") \n",
    "    sns.distplot(cvscores, label=\"cv\", color=\"slateblue\")\n",
    "    plt.legend()\n",
    "    plt.title(plottitle)\n",
    "\n",
    "plotScores(holdoutscores, cvscores, \"Model Setup 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions vs observations as scatter plot\n",
    "\n",
    "plt.scatter(y_holdout, best_model.predict(X_holdout))\n",
    "plt.xlim(-0.5,1.5)\n",
    "plt.ylim(-0.5,1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model inspection by SHAP\n",
    "\n",
    "To figure out what exactly this model is doing, performance evaluation is not sufficient. SHAP plots visualize in which direction the feature values effect the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapvals = shap.TreeExplainer(best_model).shap_values(X)\n",
    "shap.summary_plot(shapvals, X, X.columns, max_display=10, cmap=\"coolwarm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycmap = colormaps['coolwarm']\n",
    "shap.dependence_plot(\"azl\", shapvals, X, X.columns, cmap=mycmap,\n",
    "                     interaction_index=\"crop_winter_wheat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"crop_winter_wheat\", shapvals, X, X.columns,\n",
    "                     cmap=mycmap, interaction_index=\"azl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"SPEI_magnitude\", shapvals, X, X.columns, cmap=mycmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"SMI_magnitude\", shapvals, X, X.columns, cmap=mycmap,\n",
    "                     interaction_index=\"azl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
